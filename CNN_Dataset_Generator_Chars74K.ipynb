{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5759 images belonging to 36 classes.\n",
      "Found 36 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "nb_train_samples = 5759\n",
    "nb_validation_samples = 36\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "train_data_dir = 'C:/Users/JOY/Downloads/EnglishImg (1)/train'\n",
    "validation_data_dir = 'C:/Users/JOY/Downloads/EnglishImg (1)/test'\n",
    "\n",
    "# Creating our data generator for our test data\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    # used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "    rescale = 1./255)\n",
    "\n",
    "# Creating our data generator for our training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,              # normalize pixel values to [0,1]\n",
    "      #rotation_range = 30,           # randomly applies rotations\n",
    "      #width_shift_range = 0.3,       # randomly applies width shifting\n",
    "      #height_shift_range = 0.3,      # randomly applies height shifting\n",
    "      horizontal_flip = False,        # randonly flips the image\n",
    "      fill_mode = 'nearest')         # uses the fill mode nearest to fill gaps created by the above\n",
    "\n",
    "# Specify criteria about our training data, such as the directory, image size, batch size and type \n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 1,214,788\n",
      "Trainable params: 1,214,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD \n",
    "import os\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "img_rows = 150\n",
    "img_cols = 150\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(36, activation='softmax'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = SGD(0.01),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "359/359 [==============================] - 57s 159ms/step - loss: 3.4685 - accuracy: 0.0724 - val_loss: 3.5590 - val_accuracy: 0.0312\n",
      "Epoch 2/50\n",
      "359/359 [==============================] - 62s 171ms/step - loss: 3.3662 - accuracy: 0.0806 - val_loss: 3.8417 - val_accuracy: 0.0500\n",
      "Epoch 3/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 3.3376 - accuracy: 0.0888 - val_loss: 4.0379 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "359/359 [==============================] - 59s 164ms/step - loss: 3.2957 - accuracy: 0.1015 - val_loss: 3.4603 - val_accuracy: 0.0312\n",
      "Epoch 5/50\n",
      "359/359 [==============================] - 59s 164ms/step - loss: 3.1548 - accuracy: 0.1706 - val_loss: 3.6793 - val_accuracy: 0.0500\n",
      "Epoch 6/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 2.5508 - accuracy: 0.3749 - val_loss: 4.6681 - val_accuracy: 0.3500\n",
      "Epoch 7/50\n",
      "359/359 [==============================] - 60s 167ms/step - loss: 1.9822 - accuracy: 0.5003 - val_loss: 1.5460 - val_accuracy: 0.4062\n",
      "Epoch 8/50\n",
      "359/359 [==============================] - 61s 170ms/step - loss: 1.6239 - accuracy: 0.5638 - val_loss: 2.7055 - val_accuracy: 0.1500\n",
      "Epoch 9/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 1.3432 - accuracy: 0.6282 - val_loss: 3.7932 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "359/359 [==============================] - 61s 170ms/step - loss: 1.1654 - accuracy: 0.6695 - val_loss: 1.1701 - val_accuracy: 0.5625\n",
      "Epoch 11/50\n",
      "359/359 [==============================] - 60s 167ms/step - loss: 1.0698 - accuracy: 0.6897 - val_loss: 1.7978 - val_accuracy: 0.3500\n",
      "Epoch 12/50\n",
      "359/359 [==============================] - 59s 164ms/step - loss: 0.9569 - accuracy: 0.7193 - val_loss: 3.6216 - val_accuracy: 0.6500\n",
      "Epoch 13/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.8964 - accuracy: 0.7339 - val_loss: 0.9760 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "359/359 [==============================] - 59s 164ms/step - loss: 0.8443 - accuracy: 0.7503 - val_loss: 1.5514 - val_accuracy: 0.3500\n",
      "Epoch 15/50\n",
      "359/359 [==============================] - 61s 171ms/step - loss: 0.7848 - accuracy: 0.7648 - val_loss: 3.4426 - val_accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.7275 - accuracy: 0.7749 - val_loss: 0.7692 - val_accuracy: 0.6562\n",
      "Epoch 17/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.7113 - accuracy: 0.7773 - val_loss: 1.4869 - val_accuracy: 0.4000\n",
      "Epoch 18/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.6426 - accuracy: 0.7968 - val_loss: 3.9311 - val_accuracy: 0.7500\n",
      "Epoch 19/50\n",
      "359/359 [==============================] - 61s 171ms/step - loss: 0.6396 - accuracy: 0.7998 - val_loss: 0.9089 - val_accuracy: 0.6562\n",
      "Epoch 20/50\n",
      "359/359 [==============================] - 59s 164ms/step - loss: 0.6355 - accuracy: 0.7998 - val_loss: 1.2035 - val_accuracy: 0.3500\n",
      "Epoch 21/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.5953 - accuracy: 0.8114 - val_loss: 5.4134 - val_accuracy: 0.7500\n",
      "Epoch 22/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.5738 - accuracy: 0.8151 - val_loss: 0.6054 - val_accuracy: 0.6562\n",
      "Epoch 23/50\n",
      "359/359 [==============================] - 59s 166ms/step - loss: 0.5393 - accuracy: 0.8255 - val_loss: 1.1991 - val_accuracy: 0.4500\n",
      "Epoch 24/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.5265 - accuracy: 0.8348 - val_loss: 5.2779 - val_accuracy: 0.8000\n",
      "Epoch 25/50\n",
      "359/359 [==============================] - 62s 172ms/step - loss: 0.4955 - accuracy: 0.8403 - val_loss: 0.6131 - val_accuracy: 0.6875\n",
      "Epoch 26/50\n",
      "359/359 [==============================] - 58s 163ms/step - loss: 0.5018 - accuracy: 0.8379 - val_loss: 1.3918 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.4733 - accuracy: 0.8475 - val_loss: 5.5358 - val_accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "359/359 [==============================] - 62s 174ms/step - loss: 0.4578 - accuracy: 0.8492 - val_loss: 0.5945 - val_accuracy: 0.6562\n",
      "Epoch 29/50\n",
      "359/359 [==============================] - 62s 173ms/step - loss: 0.4319 - accuracy: 0.8584 - val_loss: 1.4128 - val_accuracy: 0.4500\n",
      "Epoch 30/50\n",
      "359/359 [==============================] - 60s 167ms/step - loss: 0.4336 - accuracy: 0.8553 - val_loss: 5.2483 - val_accuracy: 0.8000\n",
      "Epoch 31/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.4290 - accuracy: 0.8581 - val_loss: 0.6387 - val_accuracy: 0.6875\n",
      "Epoch 32/50\n",
      "359/359 [==============================] - 62s 173ms/step - loss: 0.4010 - accuracy: 0.8682 - val_loss: 1.4431 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "359/359 [==============================] - 61s 171ms/step - loss: 0.4030 - accuracy: 0.8696 - val_loss: 5.0538 - val_accuracy: 0.8500\n",
      "Epoch 34/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.3729 - accuracy: 0.8802 - val_loss: 0.3855 - val_accuracy: 0.7500\n",
      "Epoch 35/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.3673 - accuracy: 0.8786 - val_loss: 1.4453 - val_accuracy: 0.4000\n",
      "Epoch 36/50\n",
      "359/359 [==============================] - 61s 169ms/step - loss: 0.3650 - accuracy: 0.8797 - val_loss: 6.8835 - val_accuracy: 0.8000\n",
      "Epoch 37/50\n",
      "359/359 [==============================] - 57s 160ms/step - loss: 0.3405 - accuracy: 0.8811 - val_loss: 0.4257 - val_accuracy: 0.7812\n",
      "Epoch 38/50\n",
      "359/359 [==============================] - 56s 157ms/step - loss: 0.3491 - accuracy: 0.8846 - val_loss: 0.9653 - val_accuracy: 0.60000.3493 - accuracy: \n",
      "Epoch 39/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.3420 - accuracy: 0.8853 - val_loss: 6.5614 - val_accuracy: 0.7500\n",
      "Epoch 40/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.3280 - accuracy: 0.8933 - val_loss: 0.7273 - val_accuracy: 0.7812\n",
      "Epoch 41/50\n",
      "359/359 [==============================] - 59s 165ms/step - loss: 0.3417 - accuracy: 0.8870 - val_loss: 1.2585 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "359/359 [==============================] - 62s 174ms/step - loss: 0.3108 - accuracy: 0.8971 - val_loss: 6.4395 - val_accuracy: 0.8000\n",
      "Epoch 43/50\n",
      "359/359 [==============================] - 61s 169ms/step - loss: 0.3019 - accuracy: 0.9006 - val_loss: 0.3723 - val_accuracy: 0.8438\n",
      "Epoch 44/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.3167 - accuracy: 0.8933 - val_loss: 1.2956 - val_accuracy: 0.6500\n",
      "Epoch 45/50\n",
      "359/359 [==============================] - 60s 166ms/step - loss: 0.2703 - accuracy: 0.9086 - val_loss: 6.8418 - val_accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "359/359 [==============================] - 62s 172ms/step - loss: 0.2942 - accuracy: 0.8981 - val_loss: 0.4520 - val_accuracy: 0.7812\n",
      "Epoch 47/50\n",
      "359/359 [==============================] - 61s 170ms/step - loss: 0.2826 - accuracy: 0.9048 - val_loss: 1.4848 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "359/359 [==============================] - 61s 171ms/step - loss: 0.2770 - accuracy: 0.9049 - val_loss: 5.4120 - val_accuracy: 0.8500\n",
      "Epoch 49/50\n",
      "359/359 [==============================] - 62s 172ms/step - loss: 0.2805 - accuracy: 0.9035 - val_loss: 0.3201 - val_accuracy: 0.7812\n",
      "Epoch 50/50\n",
      "359/359 [==============================] - 61s 169ms/step - loss: 0.2649 - accuracy: 0.9119 - val_loss: 1.3776 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "dict=[]\n",
    "for i in range(0,10):\n",
    "    dict.append(str(i))\n",
    "for c in ascii_uppercase:\n",
    "    dict.append(str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/JOY/DeepLearningCV/10. Data Augmentation/ocr_augmented_91%.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('C:/Users/JOY/DeepLearningCV/10. Data Augmentation/ocr_augmented_91%.h5')\n",
    "\n",
    "def draw_test(name, pred, input_im):\n",
    "    BLACK = [0,0,0]\n",
    "    pred1=dict[pred]\n",
    "    #expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0] ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    #expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    #cv2.putText(expanded_image, str(pred), (252, 70) , cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    #cv2.imshow(name, expanded_image)\n",
    "    print(pred)\n",
    "\n",
    "input_im = cv2.imread('C:/Users/JOY/Downloads/EnglishImg (1)/test/Sample002/img002-00079.png')\n",
    "\n",
    "imageL = cv2.resize(input_im, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow(\"Test Image\", imageL)\n",
    "\n",
    "\n",
    "input_im=cv2.resize(input_im,(150,150),interpolation = cv2.INTER_AREA)\n",
    "input_im = input_im.reshape(1,150,150,3)\n",
    "input_im = input_im.astype('float32')\n",
    "input_im/=255\n",
    " \n",
    "    ## Get Prediction\n",
    "res = classifier.predict_classes(input_im, 1, verbose = 0)[0]\n",
    "print(dict[res])\n",
    "    #draw_test(\"Prediction\", res, imageL) \n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
